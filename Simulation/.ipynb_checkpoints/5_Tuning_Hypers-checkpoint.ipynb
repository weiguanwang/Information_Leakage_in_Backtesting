{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "# Append the library path to PYTHONPATH, so library can be imported.\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from library import plot\n",
    "from library import network as nw\n",
    "from library import common as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hyper Tuning data sets!\n",
      "\n",
      "Load and clean the training and validation data.\n",
      "Original data size is 24115\n",
      "We remove in-the-money samples. 13808 samples are removed. We have  42.74% of original data left.\n",
      "We shrink moneyness range. 333 samples are removed. We have  41.36% of original data left.\n",
      "\n",
      "\n",
      "====================\n",
      "Clean and load all Monte Carlo test data.\n",
      "\n",
      "We remove in-the-money samples. 2236 samples are removed. We have  40.83% of original data left.\n",
      "We shrink moneyness range. 2 samples are removed. We have  40.78% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2424 samples are removed. We have  36.41% of original data left.\n",
      "We shrink moneyness range. 67 samples are removed. We have  34.65% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2484 samples are removed. We have  39.22% of original data left.\n",
      "We shrink moneyness range. 65 samples are removed. We have  37.63% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2400 samples are removed. We have  39.49% of original data left.\n",
      "We shrink moneyness range. 79 samples are removed. We have  37.49% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2392 samples are removed. We have  40.92% of original data left.\n",
      "We shrink moneyness range. 69 samples are removed. We have  39.22% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2473 samples are removed. We have  40.55% of original data left.\n",
      "We shrink moneyness range. 15 samples are removed. We have  40.19% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2433 samples are removed. We have  40.99% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  40.99% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2314 samples are removed. We have  40.22% of original data left.\n",
      "We shrink moneyness range. 7 samples are removed. We have  40.04% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2400 samples are removed. We have  41.12% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  41.12% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2459 samples are removed. We have  39.36% of original data left.\n",
      "We shrink moneyness range. 45 samples are removed. We have  38.25% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2468 samples are removed. We have  41.18% of original data left.\n",
      "We shrink moneyness range. 61 samples are removed. We have  39.73% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2242 samples are removed. We have  40.40% of original data left.\n",
      "We shrink moneyness range. 49 samples are removed. We have  39.10% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2423 samples are removed. We have  37.90% of original data left.\n",
      "We shrink moneyness range. 148 samples are removed. We have  34.11% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2419 samples are removed. We have  39.56% of original data left.\n",
      "We shrink moneyness range. 81 samples are removed. We have  37.53% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2122 samples are removed. We have  40.41% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  40.41% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2162 samples are removed. We have  41.44% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  41.44% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2633 samples are removed. We have  38.80% of original data left.\n",
      "We shrink moneyness range. 48 samples are removed. We have  37.68% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2385 samples are removed. We have  42.41% of original data left.\n",
      "We shrink moneyness range. 3 samples are removed. We have  42.33% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2650 samples are removed. We have  36.14% of original data left.\n",
      "We shrink moneyness range. 111 samples are removed. We have  33.47% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2533 samples are removed. We have  38.23% of original data left.\n",
      "We shrink moneyness range. 116 samples are removed. We have  35.41% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2441 samples are removed. We have  41.28% of original data left.\n",
      "We shrink moneyness range. 55 samples are removed. We have  39.96% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2278 samples are removed. We have  42.05% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  42.05% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2394 samples are removed. We have  41.28% of original data left.\n",
      "We shrink moneyness range. 42 samples are removed. We have  40.25% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2535 samples are removed. We have  38.59% of original data left.\n",
      "We shrink moneyness range. 107 samples are removed. We have  36.00% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2404 samples are removed. We have  41.66% of original data left.\n",
      "We shrink moneyness range. 48 samples are removed. We have  40.50% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2599 samples are removed. We have  40.38% of original data left.\n",
      "We shrink moneyness range. 128 samples are removed. We have  37.44% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2150 samples are removed. We have  41.24% of original data left.\n",
      "We shrink moneyness range. 19 samples are removed. We have  40.72% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2476 samples are removed. We have  38.27% of original data left.\n",
      "We shrink moneyness range. 138 samples are removed. We have  34.83% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2531 samples are removed. We have  37.20% of original data left.\n",
      "We shrink moneyness range. 71 samples are removed. We have  35.43% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2515 samples are removed. We have  40.53% of original data left.\n",
      "We shrink moneyness range. 44 samples are removed. We have  39.49% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2633 samples are removed. We have  37.47% of original data left.\n",
      "We shrink moneyness range. 13 samples are removed. We have  37.16% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2339 samples are removed. We have  40.56% of original data left.\n",
      "We shrink moneyness range. 66 samples are removed. We have  38.88% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2021 samples are removed. We have  43.29% of original data left.\n",
      "We shrink moneyness range. 12 samples are removed. We have  42.96% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2418 samples are removed. We have  39.12% of original data left.\n",
      "We shrink moneyness range. 119 samples are removed. We have  36.13% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2358 samples are removed. We have  40.26% of original data left.\n",
      "We shrink moneyness range. 99 samples are removed. We have  37.75% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2278 samples are removed. We have  40.86% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  40.86% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2826 samples are removed. We have  41.31% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  41.31% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2130 samples are removed. We have  42.54% of original data left.\n",
      "We shrink moneyness range. 53 samples are removed. We have  41.11% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2219 samples are removed. We have  41.36% of original data left.\n",
      "We shrink moneyness range. 48 samples are removed. We have  40.09% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2682 samples are removed. We have  40.91% of original data left.\n",
      "We shrink moneyness range. 0 samples are removed. We have  40.91% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2792 samples are removed. We have  40.18% of original data left.\n",
      "We shrink moneyness range. 6 samples are removed. We have  40.05% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2353 samples are removed. We have  37.54% of original data left.\n",
      "We shrink moneyness range. 50 samples are removed. We have  36.21% of original data left.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We remove in-the-money samples. 2431 samples are removed. We have  41.15% of original data left.\n",
      "We shrink moneyness range. 73 samples are removed. We have  39.39% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2419 samples are removed. We have  39.37% of original data left.\n",
      "We shrink moneyness range. 51 samples are removed. We have  38.10% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2544 samples are removed. We have  38.06% of original data left.\n",
      "We shrink moneyness range. 119 samples are removed. We have  35.16% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2787 samples are removed. We have  40.77% of original data left.\n",
      "We shrink moneyness range. 3 samples are removed. We have  40.70% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2450 samples are removed. We have  39.73% of original data left.\n",
      "We shrink moneyness range. 75 samples are removed. We have  37.88% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2448 samples are removed. We have  40.84% of original data left.\n",
      "We shrink moneyness range. 132 samples are removed. We have  37.65% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2415 samples are removed. We have  39.23% of original data left.\n",
      "We shrink moneyness range. 186 samples are removed. We have  34.55% of original data left.\n",
      "\n",
      "\n",
      "We remove in-the-money samples. 2061 samples are removed. We have  42.72% of original data left.\n",
      "We shrink moneyness range. 20 samples are removed. We have  42.16% of original data left.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run setup.py\n",
    "%run Load_Clean_aux.py tune\n",
    "\n",
    "seed = 666\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune hyperparameters\n",
    "\n",
    "As an example, we tune the $L_2$ regularization strength $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Set up paths.\n",
    "Paths include data path and result path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We do not tune hypers for vix or permute.\n",
    "\"\"\"\n",
    "res_dir = f'{DATA_DIR}/Result_FREQ={OFFSET_BDAYS}_OTM={OUT_MONEY}_MINM={MIN_M}_MAXM={MAX_M}_Permute={False}_VIX={False}/'\n",
    "\n",
    "if FEATURE_SET == 'normal_feature':\n",
    "    ori_fea = ['M0', 'tau0_implvol0']\n",
    "    sub_res = res_dir + 'Network/Normal_Feature/TuneHypers/'\n",
    "\n",
    "if FEATURE_SET == 'delta_vega':\n",
    "    ori_fea = ['delta_bs', '1_over_sqrt_tau', 'vega']\n",
    "    sub_res = res_dir + 'Network/Delta_Vega/TuneHypers/'\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(sub_res, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_train.loc[df_train['period0'] == 1].copy()\n",
    "df_train = df_train.loc[df_train['period0'] == 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Standardize features\n",
    "The training and validation set are standardized, and the resulting scaler shall be passed to standardize each Monte Carlo set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fea = [x + '_t' for x in ori_fea] + ['cp_int']\n",
    "\n",
    "scaler = StandardScaler().fit(X=df_train[ori_fea])\n",
    "df_train, df_val = cm.standardize_feature([df_train, df_val], scaler, ori_fea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Specify a candidate set\n",
    "We need to specify a set of candidate hyperparameters. Each candidate will be used to train a new network for a specified number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "    'nodes_per_layer': (30, 10),\n",
    "    'reg_alpha': 1e-5,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 2\n",
    "}\n",
    "value_set = [1e-4, 1e-5, 1e-6, 1e-7]\n",
    "num_run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Weiguan\\\\Dropbox\\\\Research\\\\DeepHedging\\\\Data\\\\BlackScholes\\\\/Result_FREQ=1_OTM=True_MINM=0.8_MAXM=1.5_Permute=False_VIX=False/Network/Delta_Vega/TuneHypers/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Call `train_test_mc` function\n",
    "Each call of `train_test_mc` trains a network once and evaluate on all Monte Carlo test paths. Results are saved per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Weiguan\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Weiguan\\Anaconda3\\envs\\tfenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8528 samples, validate on 1681 samples\n",
      "Epoch 1/2\n",
      "8528/8528 [==============================] - 0s 58us/step - loss: 0.3179 - mean_squared_error: 0.3159 - val_loss: 0.0203 - val_mean_squared_error: 0.0184\n",
      "Epoch 2/2\n",
      "8528/8528 [==============================] - 0s 17us/step - loss: 0.0175 - mean_squared_error: 0.0156 - val_loss: 0.0119 - val_mean_squared_error: 0.0101\n",
      "Train on 8528 samples, validate on 1681 samples\n",
      "Epoch 1/2\n",
      "8528/8528 [==============================] - 0s 58us/step - loss: 0.0427 - mean_squared_error: 0.0425 - val_loss: 0.0090 - val_mean_squared_error: 0.0088\n",
      "Epoch 2/2\n",
      "8528/8528 [==============================] - 0s 23us/step - loss: 0.0066 - mean_squared_error: 0.0064 - val_loss: 0.0035 - val_mean_squared_error: 0.0033\n",
      "Train on 8528 samples, validate on 1681 samples\n",
      "Epoch 1/2\n",
      "8528/8528 [==============================] - 0s 43us/step - loss: 0.1291 - mean_squared_error: 0.1291 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
      "Epoch 2/2\n",
      "8528/8528 [==============================] - 0s 19us/step - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Train on 8528 samples, validate on 1681 samples\n",
      "Epoch 1/2\n",
      "8528/8528 [==============================] - 0s 43us/step - loss: 0.0408 - mean_squared_error: 0.0408 - val_loss: 0.0125 - val_mean_squared_error: 0.0125\n",
      "Epoch 2/2\n",
      "8528/8528 [==============================] - 0s 19us/step - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0050 - val_mean_squared_error: 0.0050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for value in value_set:\n",
    "    # If you want to tune other hyper apart from alpha, \n",
    "    # change key name and directory name of the next two lines.\n",
    "    hypers['reg_alpha'] = value\n",
    "    sub_dir = sub_res + 'alpha={}/'.format(value)\n",
    "    ckp_dir = sub_dir + 'ckp/'\n",
    "    hist_dir = sub_dir + 'history/'\n",
    "    metrics_dir = sub_dir + 'metrics/'\n",
    "    for _ in [ckp_folder, hist_folder, metrics_folder]:\n",
    "        os.makedirs(_, exist_ok=True)\n",
    "    \n",
    "    # for each value, we train a (new) network multiple times.\n",
    "    for i in range(num_run):   \n",
    "        \"\"\"\n",
    "        Here, each checkpoint, history, and metrics corresponds to each run.\n",
    "        \"\"\"\n",
    "        ckp_path = ckp_folder + 'bestcp{}.h5'.format(i)\n",
    "        hist_path = hist_folder + 'history{}.csv'.format(i)\n",
    "        metrics_path = metrics_folder + 'metrics{}.csv'.format(i)\n",
    "        \n",
    "        # for each iteration, we record its performance on each of MC set.\n",
    "        df_metrics = pd.DataFrame(index=range(NUM_TEST), columns=['BS', 'HN'])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        df_metrics = nw.train_test_mc(\n",
    "            df_train, df_val,\n",
    "            df_metrics=df_metrics, scaler=scaler,\n",
    "            ori_fea=ori_fea, use_fea=use_fea, \n",
    "            ckp_path=ckp_path,\n",
    "            hist_path=hist_path,\n",
    "            mc_dir=mc_dir,\n",
    "            epochs=epochs,\n",
    "            nodes_per_layer=nodes_per_layer,\n",
    "            hypers=hypers,\n",
    "            V1='V1_n',\n",
    "            dt=DT, num_test=NUM_TEST)\n",
    "        df_metrics.to_csv(metrics_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize tuning results\n",
    "This section can be run independent of the above one, if the directory paths are given properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_set = [1e-4, 1e-5, 1e-6, 1e-7]\n",
    "num_run = 1\n",
    "df_summary = pd.DataFrame(\n",
    "    index=range(num_run), \n",
    "    columns=['BS'] + ['alpha={}'.format(v) for v in value_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in value_set:\n",
    "    metrics_folder = sub_res + 'alpha={}/metrics/'.format(value)\n",
    "    for i in range(num_run):\n",
    "        df_metrics = pd.read_csv(\n",
    "            metrics_folder + 'metrics{}.csv'.format(i), index_col=0)\n",
    "        # 'BS' errors do not change, whatever the alpha is.\n",
    "        df_summary.loc[i, 'BS'] = df_metrics.mean()['BS'] \n",
    "        df_summary.loc[i, 'alpha={}'.format(value)] = df_metrics.mean()['HN']\n",
    "        \n",
    "df_summary = df_summary.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary table shows the average test metric over all the Monte Carlo data sets, for different hyperparamters and many runnings of network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv(f'{sub_res}summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BS</th>\n",
       "      <th>alpha=0.0001</th>\n",
       "      <th>alpha=1e-05</th>\n",
       "      <th>alpha=1e-06</th>\n",
       "      <th>alpha=1e-07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BS  alpha=0.0001  alpha=1e-05  alpha=1e-06  alpha=1e-07\n",
       "0  0.001434      0.010554     0.003399     0.006543     0.004796"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BS</th>\n",
       "      <th>alpha=0.0001</th>\n",
       "      <th>alpha=1e-05</th>\n",
       "      <th>alpha=1e-06</th>\n",
       "      <th>alpha=1e-07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BS  alpha=0.0001  alpha=1e-05  alpha=1e-06  alpha=1e-07\n",
       "count  1.000000      1.000000     1.000000     1.000000     1.000000\n",
       "mean   0.001434      0.010554     0.003399     0.006543     0.004796\n",
       "std         NaN           NaN          NaN          NaN          NaN\n",
       "min    0.001434      0.010554     0.003399     0.006543     0.004796\n",
       "25%    0.001434      0.010554     0.003399     0.006543     0.004796\n",
       "50%    0.001434      0.010554     0.003399     0.006543     0.004796\n",
       "75%    0.001434      0.010554     0.003399     0.006543     0.004796\n",
       "max    0.001434      0.010554     0.003399     0.006543     0.004796"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BS</th>\n",
       "      <th>alpha=0.0001</th>\n",
       "      <th>alpha=1e-05</th>\n",
       "      <th>alpha=1e-06</th>\n",
       "      <th>alpha=1e-07</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>&amp;1.000</td>\n",
       "      <td>&amp;1.000</td>\n",
       "      <td>&amp;1.000</td>\n",
       "      <td>&amp;1.000</td>\n",
       "      <td>&amp;1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>&amp;0.143</td>\n",
       "      <td>&amp;1.055</td>\n",
       "      <td>&amp;0.340</td>\n",
       "      <td>&amp;0.654</td>\n",
       "      <td>&amp;0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>&amp;nan</td>\n",
       "      <td>&amp;nan</td>\n",
       "      <td>&amp;nan</td>\n",
       "      <td>&amp;nan</td>\n",
       "      <td>&amp;nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>&amp;0.143</td>\n",
       "      <td>&amp;1.055</td>\n",
       "      <td>&amp;0.340</td>\n",
       "      <td>&amp;0.654</td>\n",
       "      <td>&amp;0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>&amp;0.143</td>\n",
       "      <td>&amp;1.055</td>\n",
       "      <td>&amp;0.340</td>\n",
       "      <td>&amp;0.654</td>\n",
       "      <td>&amp;0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>&amp;0.143</td>\n",
       "      <td>&amp;1.055</td>\n",
       "      <td>&amp;0.340</td>\n",
       "      <td>&amp;0.654</td>\n",
       "      <td>&amp;0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>&amp;0.143</td>\n",
       "      <td>&amp;1.055</td>\n",
       "      <td>&amp;0.340</td>\n",
       "      <td>&amp;0.654</td>\n",
       "      <td>&amp;0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>&amp;0.143</td>\n",
       "      <td>&amp;1.055</td>\n",
       "      <td>&amp;0.340</td>\n",
       "      <td>&amp;0.654</td>\n",
       "      <td>&amp;0.480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BS  alpha=0.0001  alpha=1e-05  alpha=1e-06  alpha=1e-07\n",
       "count &1.000        &1.000       &1.000       &1.000       &1.000\n",
       "mean  &0.143        &1.055       &0.340       &0.654       &0.480\n",
       "std     &nan          &nan         &nan         &nan         &nan\n",
       "min   &0.143        &1.055       &0.340       &0.654       &0.480\n",
       "25%   &0.143        &1.055       &0.340       &0.654       &0.480\n",
       "50%   &0.143        &1.055       &0.340       &0.654       &0.480\n",
       "75%   &0.143        &1.055       &0.340       &0.654       &0.480\n",
       "max   &0.143        &1.055       &0.340       &0.654       &0.480"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '&{:,.3f}'.format\n",
    "(df_summary * 100).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results\n",
    "Run this section, if you want to save all related checkpoint, history and plots permanently, so that they will not be covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data setup file from the clean data folder, and then append network setup.\n",
    "shutil.copy(f'{data_dir}paras.txt', sub_res)\n",
    "\n",
    "with open(f'{sub_res}paras.txt', 'a') as file:\n",
    "    file.write('\\n\\nThe following is TUNE setup.\\n')\n",
    "    file.write(f'Date and time = {datetime.datetime.now()}\\n')\n",
    "    for n, x in [\n",
    "        ('Random seed', seed),\n",
    "        ('Features used', use_fea),\n",
    "        ('Learning rate', hypers['lr']),\n",
    "        ('Value set to tune from', value_set),\n",
    "        ('Nodes per layer', nodes_per_layer),\n",
    "        ('Number of training epochs', epochs),\n",
    "        ('Number of iterations', num_run)\n",
    "    ]:\n",
    "        file.write(f'{n} = {x}\\n') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Weiguan\\\\Dropbox\\\\Research\\\\DeepHedging\\\\Data\\\\BlackScholes\\\\Result/Network/Normal_feature/TuneHypers/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
