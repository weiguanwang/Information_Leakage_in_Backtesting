{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Append the library path to PYTHONPATH, so library can be imported.\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from library import plot\n",
    "from library import vix\n",
    "from library import common as cm\n",
    "from library import cleaner_aux as caux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.py\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(RANDOM_SEED)   # only used for VIX; should be moved there sometimes\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = DATA_DIR + 'RawData/'\n",
    "clean_dir = DATA_DIR + 'CleanData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Clean data saved at: {clean_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Import data\n",
    "We here import option trading data, S&P 500 index, zero coupon yield (for maturitity larger than one day), and overnight rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option data file.\n",
    "df_op = pd.read_csv(\n",
    "    raw_dir + 'option_price.csv', \n",
    "    usecols=['date', 'exdate', 'strike_price', 'impl_volatility', 'delta',\n",
    "             'gamma', 'theta', 'vega', 'optionid', 'best_bid', \n",
    "             'best_offer', 'volume', 'cp_flag', 'last_date'],\n",
    "    dtype={'cp_flag': 'category'},\n",
    "    parse_dates=['date', 'exdate', 'last_date'], infer_datetime_format=True)\n",
    "\n",
    "ori_size = df_op.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We rename several columns to make names match with simulation data.\n",
    "df_op.rename(\n",
    "    columns={'strike_price': 'K',\n",
    "            'impl_volatility': 'implvol0',\n",
    "            'delta': 'delta_bs'},\n",
    "    inplace=True)\n",
    "df_op['K'] /= 1000\n",
    "\n",
    "df_op['cp_flag'].cat.rename_categories({\n",
    "    'C': 'OC',\n",
    "    'P': 'OP'}, \n",
    "    inplace=True)\n",
    "df_op['cp_int'] = 0.\n",
    "df_op.loc[df_op['cp_flag'] == 'OP', 'cp_int'] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index data file.\n",
    "df_spx = pd.read_csv(\n",
    "    raw_dir + 'spx500.csv', \n",
    "    usecols=['date', 'close'], \n",
    "    index_col=0,\n",
    "    parse_dates=['date'], dayfirst=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero coupon yield rate.\n",
    "df_r = pd.read_csv(\n",
    "    raw_dir + 'interest_rate.csv',\n",
    "    parse_dates=['date'], dayfirst=False)\n",
    "# We exclude rates for days equal to 1, since they should be from overnight rate.\n",
    "print('The number of rates whose number of days equal to 1 is {}'.format((df_r['days'] == 1).sum()))\n",
    "df_r = df_r.loc[~(df_r['days'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overnight rate file.\n",
    "df_onr = pd.read_csv(\n",
    "    raw_dir + 'onr.csv', \n",
    "    header=0, parse_dates=['date'], dayfirst=True)\n",
    "df_onr.sort_values('date', inplace=True)\n",
    "df_onr.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the overnight rate file with the zero coupon yield file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only retrieve overnight rates when there exist option trades.\n",
    "# When there is not a matching over night rate, the previous valid value is propagated forward.\n",
    "df_onr = df_onr.reindex(index=np.sort(df_op['date'].unique()), method='pad')\n",
    "\n",
    "df_onr.reset_index(inplace=True)\n",
    "df_rates = pd.concat([df_r, df_onr], sort=True)\n",
    "df_rates = df_rates.sort_values(by=['date', 'days'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step : Merge dataframes\n",
    "1. we need to find the current index level for each option quote. \n",
    "2. we need to find the the one-step-ahead (or multiple-step) option price, index level, and implied volatility for each option trade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['V0'] = (df_op['best_bid'] + df_op['best_offer']) / 2\n",
    "df_op['S0'] = df_spx.reindex(index=df_op['date']).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Default columns to be joined are S1, V1, implvol1, future volume \"\"\"\n",
    "for key, value in OFFSET_DICT.items():\n",
    "    df_op = caux.add_tomorrow(\n",
    "        df_op, \n",
    "        offset_bday=value[0], offset_key=value[1], future_vol=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step : Remove certain types of data\n",
    "Some data from original file should be removed. We list those types we will exclude here. All the operations here work on the option file only. There will be additional cleaning after underlying prices are appended.\n",
    "1. Quotes with zero volume.\n",
    "2. Bid-ask spread larger than 50% of best offer price.\n",
    "3. Remove quotes with bid smaller than 0.05 (0.05 is one tick)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ori_size = df_op.shape[0]\n",
    "print(f'Original size is {ori_size}')\n",
    "bl = df_op['volume'] >= 1\n",
    "cm.print_removal(df_op.shape[0], sum(bl), ori_size, 'We remove samples with zero volume')\n",
    "df_op = df_op.loc[bl]\n",
    "\n",
    "bl = (df_op['best_offer'] <= 2* df_op['best_bid'])\n",
    "cm.print_removal(df_op.shape[0], sum(bl), ori_size, 'We remove samples that ask is larger than 2 * bid')\n",
    "df_op = df_op.loc[bl]\n",
    "\n",
    "bl = df_op['best_bid'] > 0.049\n",
    "cm.print_removal(df_op.shape[0], sum(bl), ori_size, 'We remove samples with bid smaller than 0.05')\n",
    "df_op = df_op.loc[bl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Interest rate to expiry\n",
    "Proper interest rates to use for discounting should have the same maturity as the option's maturity. Therefore, we need to match the interest rate with option expiry. Here we first interpolate interest rate for various length for each date, and then multiple index the correct rate.\n",
    "\n",
    "In addition, we need interest rate that corresponds to hedge frequency to calculate risk free asset value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['tau0'] = caux.get_time2maturity(df_op['date'], df_op['exdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of calender days to expiry\n",
    "c_days = map(lambda x: x.days, df_op['exdate'] - df_op['date'])\n",
    "c_days = list(c_days)\n",
    "\n",
    "groups = df_rates.groupby('date')\n",
    "max_days = (df_op['exdate'] - df_op['date']).max().days\n",
    "num_days = np.arange(1, max_days + 5)\n",
    "\n",
    "res_rates = pd.DataFrame()\n",
    "for key, group in groups:\n",
    "    res = caux.interp_rate2expiry(group, num_days)\n",
    "    res['date'] = key.date()\n",
    "    res_rates = res_rates.append(res)\n",
    "\n",
    "res_rates = res_rates.set_index(['date', 'days'])\n",
    "\n",
    "df_op['r'] = 0.\n",
    "df_op['r'] = res_rates.reindex(\n",
    "    index=list(zip(df_op['date'], c_days))).values / 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the rate used for option pricing, here, We also need to know the short rate, which is used to calculate the future value of risk-free asset. To simplify, we only use the overnight rate only, no matter how many days the hedging offset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we got short rate. \n",
    "Then continuous discounting should be applied when a hedging offset is chosen when loading the data set.\n",
    "\"\"\"\n",
    "df_op[f'short_rate'] = res_rates.reindex(\n",
    "    index=list(zip(df_op['date'], [1] * len(df_op['date'])))).values / 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Additional cleaning\n",
    "Here we clean based on option pricing common knowledge.\n",
    "1. We remove option quotes whose time value is negative.\n",
    "2. We restrict the range of acceptable moneyness. To be specific, we exclude deep in the money calls and deep in the money puts.\n",
    "3. We remove option quotes whose time value are negative.\n",
    "3. We tag data issues, e.g., when the last trade data is not current date, and when delta is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl =  (df_op['cp_flag'] == 'OC') & (df_op['S0'] - np.exp(-df_op['r'] * df_op['tau0']) * df_op['K'] >= df_op['V0'])\n",
    "cm.print_removal(df_op.shape[0], sum(~bl), ori_size, 'We remove CALL samples with negative time value')\n",
    "df_op = df_op.loc[~bl]\n",
    "\n",
    "bl =  (df_op['cp_flag'] == 'OP') & (np.exp(-df_op['r'] * df_op['tau0']) * df_op['K'] - df_op['S0'] >= df_op['V0'])\n",
    "cm.print_removal(df_op.shape[0], sum(~bl), ori_size, 'We remove PUT samples with negative time value')\n",
    "df_op = df_op.loc[~bl]\n",
    "\n",
    "df_op['M0'] = df_op['S0'] / df_op['K']\n",
    "bl =  (df_op['cp_flag'] == 'OC') & (df_op['M0'] > 2.0)\n",
    "cm.print_removal(df_op.shape[0], sum(~bl), ori_size, 'We remove deep in the money call samples')\n",
    "df_op = df_op[~bl]\n",
    "\n",
    "bl =  (df_op['cp_flag'] == 'OP') & (df_op['M0'] < 0.5)\n",
    "cm.print_removal(df_op.shape[0], sum(~bl), ori_size, 'We remove deep in the money put samples')\n",
    "df_op = df_op[~bl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['Code_dataissue'] = 0\n",
    "\n",
    "bl = (df_op['date'] != df_op['last_date'])\n",
    "df_op.loc[bl, 'Code_dataissue'] = 1\n",
    "print('Warning: Number of samples with last_trade_date issues: {}'.format(bl.sum()))\n",
    "\n",
    "bl = df_op['delta_bs'].isnull()\n",
    "df_op.loc[bl, 'Code_dataissue'] = df_op.loc[bl, 'Code_dataissue'] + 2\n",
    "print('Warning: Number of samples with missing delta: {}'.format(bl.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_c = df_op['cp_flag'] == 'OC'\n",
    "bl_p = df_op['cp_flag'] == 'OP'\n",
    "df_op.loc[bl_c, 'cp_int'] = 0\n",
    "df_op.loc[bl_p, 'cp_int'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6: Calculate implied vol and delta, gamma and vega, and Normalize prices.\n",
    "OptionMetrics provides implied volatlity and delta, but some are missing. We calculate all the implied volatility, greeks, assuming zero dividend rate. We will choose how to use our own calculated number when loading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['implvol0', 'delta_bs', 'gamma', 'vega']:\n",
    "    df_op.rename(columns={x: x + '_o'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op = caux.calc_implvol(df_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op.rename(columns={'implvol0': 'implvol0_c'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = (['S' + value[1] for key, value in OFFSET_DICT.items()]\n",
    "                     + ['V' + value[1] for key, value in OFFSET_DICT.items()])\n",
    "\n",
    "df_op = caux.normalize_prices(\n",
    "    df_op, df_op['S0'], NORM_FACTOR, \n",
    "    cols=['S0', 'V0', 'K'] + cols_to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['delta_bs_c'] = caux.bs_call_delta(\n",
    "    vol=df_op['implvol0_c'], S=df_op['S0_n'], K=df_op['K_n'], tau=df_op['tau0'], r=df_op['r'])\n",
    "df_op.loc[bl_p, 'delta_bs_c'] -= 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_op['gamma_c_n'] = caux.bs_gamma(\n",
    "    vol=df_op['implvol0_c'], S=df_op['S0_n'], K=df_op['K_n'], tau=df_op['tau0'], r=df_op['r'])\n",
    "df_op['vega_c_n'] = caux.bs_vega(\n",
    "    vol=df_op['implvol0_c'], S=df_op['S0_n'], K=df_op['K_n'], tau=df_op['tau0'], r=df_op['r'])\n",
    "df_op['vanna_c_n'] = caux.bs_vanna(\n",
    "    vol=df_op['implvol0_c'], S=df_op['S0_n'], K=df_op['K_n'], tau=df_op['tau0'], r=df_op['r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In addition to gamma calculated from our own implied volatility,\n",
    "we also want to have normalized gamma (vega) from original gamma (vega).\n",
    "We only normalize when they are available. \n",
    "The normalization calculation is a bit different; no longer calculating from scratch\n",
    "\"\"\"\n",
    "df_op['gamma_o_n'] = df_op['gamma_o'] * df_op['S0'] / NORM_FACTOR\n",
    "df_op['vega_o_n'] = df_op['vega_o'] / df_op['S0'] * NORM_FACTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Add VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vix_path = vix.simulate_fake_vix(VIXPARAS, DATE_BEGIN, DATE_END)\n",
    "df_op = df_op.join(vix_path, on='date')\n",
    "vix_path.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(clean_dir, exist_ok=True)\n",
    "df_op.to_csv(clean_dir + 'processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of samples stored: {}'.format(len(df_op)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy('setup.py', f'{clean_dir}setup.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{clean_dir}/paras.txt', 'w+') as file:\n",
    "    for n, x in [\n",
    "    \t('Random seed', RANDOM_SEED),\n",
    "    \t('Normalized price', NORM_FACTOR),\n",
    "    \t('First date', DATE_BEGIN),\n",
    "    \t('Last date', DATE_END),\n",
    "        ('Number of samples in total', len(df_op))]:\n",
    "        file.write(f'{n} = {x}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
