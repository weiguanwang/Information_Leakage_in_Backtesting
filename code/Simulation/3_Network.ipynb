{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Append the library path to PYTHONPATH, so library can be imported.\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from library import plot, bs\n",
    "from library import network as nw\n",
    "from library import common as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Normal data sets!\n",
      "\n",
      "Load and clean the training and validation data.\n",
      "Original data size is 756804\n",
      "We remove in-the-money samples. 387229 samples (51.17%) are removed. We have 48.83% of original data left, yielding a size of 369575.\n",
      "We shrink moneyness range. 0 samples (0.00%) are removed. We have 48.83% of original data left, yielding a size of 369575.\n",
      "We remove samples when S1 is not available. 1404 samples (0.38%) are removed. We have 48.65% of original data left, yielding a size of 368171.\n",
      "\n",
      "\n",
      "====================\n",
      "Clean and load all Monte Carlo test data.\n",
      "\n",
      "Load Monte Carlo set 1\n",
      "We remove in-the-money samples. 107339 samples (52.08%) are removed. We have 47.92% of original data left, yielding a size of 98752.\n",
      "We shrink moneyness range. 0 samples (0.00%) are removed. We have 47.92% of original data left, yielding a size of 98752.\n",
      "We remove samples when S1 is not available. 1965 samples (1.99%) are removed. We have 46.96% of original data left, yielding a size of 96787.\n",
      "\n",
      "\n",
      "Load Monte Carlo set 2\n",
      "We remove in-the-money samples. 93677 samples (51.06%) are removed. We have 48.94% of original data left, yielding a size of 89782.\n",
      "We shrink moneyness range. 0 samples (0.00%) are removed. We have 48.94% of original data left, yielding a size of 89782.\n",
      "We remove samples when S1 is not available. 1432 samples (1.59%) are removed. We have 48.16% of original data left, yielding a size of 88350.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run setup.py\n",
    "%run Load_Clean_aux.py normal\n",
    "\n",
    "seed = 666\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "    'nodes_per_layer': (30, 30),\n",
    "    'reg_alpha': 1e-4,\n",
    "    'lr': 1e-4,\n",
    "    'epochs': 5, #1000\n",
    "    'outact': 'linear'\n",
    "}\n",
    "ori_fea_save = ['delta_bs', '1_over_sqrt_tau', 'vega_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note here, we use the same directory structure for the permuted and non-permuted data. \n",
    "\"\"\"\n",
    "def create_sub_res_dirs(sub_res):\n",
    "    sub_res_dirs = {\n",
    "        'ckp': sub_res + 'ckp/',\n",
    "        'history': sub_res + 'history/',\n",
    "        'pnl': sub_res + 'pnl/',\n",
    "        'plot': sub_res + 'plot/'\n",
    "    }\n",
    "    for key, value in sub_res_dirs.items():\n",
    "        os.makedirs(value, exist_ok=True)\n",
    "    shutil.copy('setup.py', sub_res)\n",
    "    return sub_res_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERMUTE = False\n",
    "df_val_save = df_train.loc[df_train['period0'] == 1].copy()\n",
    "df_train_save = df_train.loc[df_train['period0'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The loaded `df_train` set contains both the training and the validation set. So we need to split.\n",
    "\n",
    "for VIX in [False, True]:\n",
    "    \n",
    "    # Because df_train and df_val will be modified afterwards (by feature standarize),\n",
    "    # so we need to initialise at the beginning for VIX=True.\n",
    "    # The previous LR does not have such issue\n",
    "    df_train = df_train_save.copy()\n",
    "    df_val = df_val_save.copy()\n",
    "    \n",
    "    \"\"\" Step 1: Manage Paths\n",
    "    For the non-permuted data:\n",
    "    The network is only trained once. so the ckp and the history are from that training.\n",
    "    Each of the pnls is for each different monte carlo, but they come from the same network.\n",
    "    \"\"\"\n",
    "    \n",
    "    if VIX:\n",
    "        ori_fea = ori_fea_save + ['fake_vix']\n",
    "    else:\n",
    "        ori_fea = ori_fea_save\n",
    "    res_dir = f'{DATA_DIR}Result/CONFIG={CONFIG}/FREQ={FREQ}_HALFMONEY={HALF_MONEY}_MINM={MIN_M}_MAXM={MAX_M}_Permute={PERMUTE}_VIX={VIX}/'\n",
    "    sub_res = res_dir + 'Network/Delta_Vega/'   \n",
    "    sub_res_dirs = create_sub_res_dirs(sub_res)\n",
    "    \n",
    "    \"\"\"\n",
    "    ##### Step 2: Choose feature and standardize\n",
    "    Before data sets are fed to a network, all their features need to be standardized to \n",
    "    have zero mean and unit standard deviation.\n",
    "    \"\"\"\n",
    "    use_fea = [x + '_t' for x in ori_fea] + ['cp_int']\n",
    "\n",
    "    scaler = StandardScaler().fit(X=df_train[ori_fea])\n",
    "    df_train, df_val = nw.standardize_feature([df_train, df_val], scaler, ori_fea)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Step 3: Build a network and train it\n",
    "    \"\"\"\n",
    "    sub_res_paths = {\n",
    "        'ckp': sub_res_dirs['ckp'] + 'bestcp.h5',\n",
    "        'history': sub_res_dirs['history'] + 'history.csv',\n",
    "        'plot': sub_res_dirs['plot'] + 'losscurve.png'\n",
    "    }\n",
    "    if TRAIN_BY_YOURSELF is True:    \n",
    "        history = nw.train_net_core(df_train, df_val, use_fea, hypers, sub_res_paths)    \n",
    "        nw.plot_history(history, sub_res_paths['plot'], df_train, df_val)\n",
    "    else:\n",
    "        # use the provided checkpoints\n",
    "        sub_res_paths['ckp'] = f'{CKP_DIR}Permute={PERMUTE}_VIX={VIX}/bestcp.h5'\n",
    "\n",
    "    for i in range(NUM_TEST):\n",
    "        df_test = mc_sets[i]\n",
    "\n",
    "        [df_test] = nw.standardize_feature([df_test], scaler, ori_fea)\n",
    "        delta = nw.test_net_core(df_test, use_fea, sub_res_paths)\n",
    "\n",
    "        cm.store_pnl(\n",
    "            df_test, delta,\n",
    "            pnl_path=sub_res_dirs['pnl'] + f'pnl{i}.csv'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Normal data sets!\n",
      "\n",
      "Load and clean the training and validation data.\n",
      "Original data size is 756804\n",
      "We remove in-the-money samples. 387229 samples (51.17%) are removed. We have 48.83% of original data left, yielding a size of 369575.\n",
      "We shrink moneyness range. 0 samples (0.00%) are removed. We have 48.83% of original data left, yielding a size of 369575.\n",
      "We remove samples when S1 is not available. 1404 samples (0.38%) are removed. We have 48.65% of original data left, yielding a size of 368171.\n",
      "\n",
      "\n",
      "====================\n",
      "Clean and load all Monte Carlo test data.\n",
      "\n",
      "Load Monte Carlo set 1\n",
      "We remove in-the-money samples. 107339 samples (52.08%) are removed. We have 47.92% of original data left, yielding a size of 98752.\n",
      "We shrink moneyness range. 0 samples (0.00%) are removed. We have 47.92% of original data left, yielding a size of 98752.\n",
      "We remove samples when S1 is not available. 1965 samples (1.99%) are removed. We have 46.96% of original data left, yielding a size of 96787.\n",
      "\n",
      "\n",
      "Load Monte Carlo set 2\n",
      "We remove in-the-money samples. 93677 samples (51.06%) are removed. We have 48.94% of original data left, yielding a size of 89782.\n",
      "We shrink moneyness range. 0 samples (0.00%) are removed. We have 48.94% of original data left, yielding a size of 89782.\n",
      "We remove samples when S1 is not available. 1432 samples (1.59%) are removed. We have 48.16% of original data left, yielding a size of 88350.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PERMUTE = True\n",
    "%run Load_Clean_aux.py normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare permutations.\n",
    "\"\"\"\n",
    "\n",
    "train_permutes, val_permutes, test_permutes = [], [], []\n",
    "\n",
    "for i in range(NUM_TEST):\n",
    "    # the union of train and test\n",
    "    df_permute = df_train.append(mc_sets[i], ignore_index=True, sort=False)\n",
    "    df_permute = cm.permute_core(df_permute, 0, random_seed=i)\n",
    "\n",
    "    df_train_permuted = df_permute.loc[df_permute['period0'] == 0]\n",
    "    df_val_permuted = df_permute.loc[df_permute['period0'] == 1]\n",
    "    df_test_permuted = df_permute.loc[df_permute['period0'] == 2]\n",
    "\n",
    "    train_permutes.append(df_train_permuted.copy())\n",
    "    val_permutes.append(df_val_permuted.copy())\n",
    "    test_permutes.append(df_test_permuted.copy())\n",
    "    \n",
    "del mc_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for VIX in [False, True]:\n",
    "    \"\"\"\n",
    "    For the permuted data:\n",
    "    The network is trained for the number of permutations. \n",
    "    Each of the pnls is for each permuations, and they comes from each trained network.\n",
    "    \"\"\"\n",
    "    \n",
    "    if VIX:\n",
    "        ori_fea = ori_fea_save + ['fake_vix']\n",
    "    else:\n",
    "        ori_fea = ori_fea_save   \n",
    "    res_dir = f'{DATA_DIR}Result/CONFIG={CONFIG}/FREQ={FREQ}_HALFMONEY={HALF_MONEY}_MINM={MIN_M}_MAXM={MAX_M}_Permute={PERMUTE}_VIX={VIX}/'\n",
    "    sub_res = res_dir + 'Network/Delta_Vega/' \n",
    "    sub_res_dirs = create_sub_res_dirs(sub_res)\n",
    "    \n",
    "    for i in range(NUM_TEST):\n",
    "        \"\"\"\n",
    "        ##### Step 2: Choose feature and standardize\n",
    "        The difference of permuating version and the above version is:\n",
    "        we standardize for each permutation.\n",
    "        \"\"\"\n",
    "        use_fea = [x + '_t' for x in ori_fea] + ['cp_int']\n",
    "        \n",
    "        scaler = StandardScaler().fit(X=train_permutes[i][ori_fea])\n",
    "        train_temp, val_temp = nw.standardize_feature([train_permutes[i], val_permutes[i]], scaler, ori_fea)\n",
    "\n",
    "        \"\"\"\n",
    "        ##### Step 3: Build a network and train it\n",
    "        \"\"\"\n",
    "        sub_res_paths = {\n",
    "            'ckp': sub_res_dirs['ckp'] + f'bestcp{i}.h5',\n",
    "            'history': sub_res_dirs['history'] + f'history{i}.csv',\n",
    "            'plot': sub_res_dirs['plot'] + f'losscurve{i}.png'\n",
    "        }\n",
    "        if TRAIN_BY_YOURSELF is True: \n",
    "            history = nw.train_net_core(train_temp, val_temp, use_fea, hypers, sub_res_paths)\n",
    "            nw.plot_history(history, sub_res_paths['plot'], train_temp, val_temp)\n",
    "        else:\n",
    "            # use the provided checkpoints\n",
    "            sub_res_paths['ckp'] = f'{CKP_DIR}Permute={PERMUTE}_VIX={VIX}/bestcp{i}.h5'\n",
    "\n",
    "        \"\"\"\n",
    "        Test the network for only one permuted test set.\n",
    "        \"\"\"\n",
    "        [test_temp] = nw.standardize_feature([test_permutes[i]], scaler, ori_fea)\n",
    "        delta = nw.test_net_core(test_temp, use_fea, sub_res_paths)\n",
    "        cm.store_pnl(\n",
    "            test_temp, delta,\n",
    "            pnl_path=sub_res_dirs['pnl'] + f'pnl{i}.csv'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{sub_res}additional_paras.txt', 'w+') as file:\n",
    "    file.write('The following is network setup.\\n')\n",
    "    file.write(f'Date and time = {datetime.datetime.now()}\\n')\n",
    "    for n, x in [\n",
    "        ('Random seed', seed),\n",
    "        ('Features used', use_fea),\n",
    "        ('Learning rate', hypers['lr']),\n",
    "        ('L2 regularization alpha', hypers['reg_alpha']),\n",
    "        ('Nodes per layer', hypers['nodes_per_layer']),\n",
    "        ('Number of training epochs', hypers['epochs'])\n",
    "    ]:\n",
    "        file.write(f'{n} = {x}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
